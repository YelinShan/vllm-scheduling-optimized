INFO 06-17 16:38:27 [__init__.py:244] Automatically detected platform cuda.
usage: benchmark_serving.py [-h]
                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,openai-audio,tensorrt-llm,scalellm,sglang,llama.cpp}]
                            [--base-url BASE_URL] [--host HOST] [--port PORT]
                            [--endpoint ENDPOINT]
                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf,custom}]
                            [--dataset-path DATASET_PATH]
                            [--max-concurrency MAX_CONCURRENCY] --model MODEL
                            [--tokenizer TOKENIZER] [--use-beam-search]
                            [--num-prompts NUM_PROMPTS] [--logprobs LOGPROBS]
                            [--request-rate REQUEST_RATE]
                            [--burstiness BURSTINESS] [--seed SEED]
                            [--trust-remote-code] [--disable-tqdm] [--profile]
                            [--save-result] [--save-detailed]
                            [--append-result] [--metadata [KEY=VALUE ...]]
                            [--result-dir RESULT_DIR]
                            [--result-filename RESULT_FILENAME] [--ignore-eos]
                            [--percentile-metrics PERCENTILE_METRICS]
                            [--metric-percentiles METRIC_PERCENTILES]
                            [--goodput GOODPUT [GOODPUT ...]]
                            [--custom-output-len CUSTOM_OUTPUT_LEN]
                            [--custom-skip-chat-template]
                            [--sonnet-input-len SONNET_INPUT_LEN]
                            [--sonnet-output-len SONNET_OUTPUT_LEN]
                            [--sonnet-prefix-len SONNET_PREFIX_LEN]
                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]
                            [--random-input-len RANDOM_INPUT_LEN]
                            [--random-output-len RANDOM_OUTPUT_LEN]
                            [--random-range-ratio RANDOM_RANGE_RATIO]
                            [--random-prefix-len RANDOM_PREFIX_LEN]
                            [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]
                            [--hf-output-len HF_OUTPUT_LEN] [--top-p TOP_P]
                            [--top-k TOP_K] [--min-p MIN_P]
                            [--temperature TEMPERATURE]
                            [--tokenizer-mode {auto,slow,mistral,custom}]
                            [--served-model-name SERVED_MODEL_NAME]
                            [--lora-modules LORA_MODULES [LORA_MODULES ...]]

Benchmark the online serving throughput.

options:
  --append-result       Append the benchmark result to the existing json file.
                        (default: False)
  --backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,openai-audio,tensorrt-llm,scalellm,sglang,llama.cpp}
  --base-url BASE_URL   Server or API base url if not using http host and
                        port. (default: None)
  --burstiness BURSTINESS
                        Burstiness factor of the request generation. Only take
                        effect when request_rate is not inf. Default value is
                        1, which follows Poisson process. Otherwise, the
                        request intervals follow a gamma distribution. A lower
                        burstiness value (0 < burstiness < 1) results in more
                        bursty requests. A higher burstiness value (burstiness
                        > 1) results in a more uniform arrival of requests.
                        (default: 1.0)
  --dataset-name {sharegpt,burstgpt,sonnet,random,hf,custom}
                        Name of the dataset to benchmark on. (default:
                        sharegpt)
  --dataset-path DATASET_PATH
                        Path to the sharegpt/sonnet dataset. Or the
                        huggingface dataset ID if using HF dataset. (default:
                        None)
  --disable-tqdm        Specify to disable tqdm progress bar. (default: False)
  --endpoint ENDPOINT   API endpoint. (default: /v1/completions)
  --goodput GOODPUT [GOODPUT ...]
                        Specify service level objectives for goodput as
                        "KEY:VALUE" pairs, where the key is a metric name, and
                        the value is in milliseconds. Multiple "KEY:VALUE"
                        pairs can be provided, separated by spaces. Allowed
                        request level metric names are "ttft", "tpot", "e2el".
                        For more context on the definition of goodput, refer
                        to DistServe paper: https://arxiv.org/pdf/2401.09670
                        and the blog: https://hao-ai-
                        lab.github.io/blogs/distserve (default: None)
  --host HOST
  --ignore-eos          Set ignore_eos flag when sending the benchmark
                        request.Warning: ignore_eos is not supported in
                        deepspeed_mii and tgi. (default: False)
  --logprobs LOGPROBS   Number of logprobs-per-token to compute & return as
                        part of the request. If unspecified, then either (1)
                        if beam search is disabled, no logprobs are computed &
                        a single dummy logprob is returned for each token; or
                        (2) if beam search is enabled 1 logprob per token is
                        computed (default: None)
  --lora-modules LORA_MODULES [LORA_MODULES ...]
                        A subset of LoRA module names passed in when launching
                        the server. For each request, the script chooses a
                        LoRA module at random. (default: None)
  --max-concurrency MAX_CONCURRENCY
                        Maximum number of concurrent requests. This can be
                        used to help simulate an environment where a higher
                        level component is enforcing a maximum number of
                        concurrent requests. While the --request-rate argument
                        controls the rate at which requests are initiated,
                        this argument will control how many are actually
                        allowed to execute at a time. This means that when
                        used in combination, the actual request rate may be
                        lower than specified with --request-rate, if the
                        server is not processing requests fast enough to keep
                        up. (default: None)
  --metadata [KEY=VALUE ...]
                        Key-value pairs (e.g, --metadata version=0.3.3 tp=1)
                        for metadata of this run to be saved in the result
                        JSON file for record keeping purposes. (default: None)
  --metric-percentiles METRIC_PERCENTILES
                        Comma-separated list of percentiles for selected
                        metrics. To report 25-th, 50-th, and 75-th
                        percentiles, use "25,50,75". Default value is "99".
                        Use "--percentile-metrics" to select metrics.
                        (default: 99)
  --model MODEL         Name of the model. (default: None)
  --num-prompts NUM_PROMPTS
                        Number of prompts to process. (default: 1000)
  --percentile-metrics PERCENTILE_METRICS
                        Comma-separated list of selected metrics to report
                        percentils. This argument specifies the metrics to
                        report percentiles. Allowed metric names are "ttft",
                        "tpot", "itl", "e2el". Default value is
                        "ttft,tpot,itl". (default: ttft,tpot,itl)
  --port PORT
  --profile             Use Torch Profiler. The endpoint must be launched with
                        VLLM_TORCH_PROFILER_DIR to enable profiler. (default:
                        False)
  --request-rate REQUEST_RATE
                        Number of requests per second. If this is inf, then
                        all the requests are sent at time 0. Otherwise, we use
                        Poisson process or gamma distribution to synthesize
                        the request arrival times. (default: inf)
  --result-dir RESULT_DIR
                        Specify directory to save benchmark json results.If
                        not specified, results are saved in the current
                        directory. (default: None)
  --result-filename RESULT_FILENAME
                        Specify the filename to save benchmark json results.If
                        not specified, results will be saved in {backend}-
                        {args.request_rate}qps-{base_model_id}-
                        {current_dt}.json format. (default: None)
  --save-detailed       When saving the results, whether to include per
                        request information such as response, error, ttfs,
                        tpots, etc. (default: False)
  --save-result         Specify to save benchmark results to a json file
                        (default: False)
  --seed SEED
  --served-model-name SERVED_MODEL_NAME
                        The model name used in the API. If not specified, the
                        model name will be the same as the ``--model``
                        argument.  (default: None)
  --tokenizer TOKENIZER
                        Name or path of the tokenizer, if not using the
                        default tokenizer. (default: None)
  --tokenizer-mode {auto,slow,mistral,custom}
                        The tokenizer mode.
                        * "auto" will use the fast tokenizer if available. *
                        "slow" will always use the slow tokenizer.  *
                        "mistral" will always use the `mistral_common`
                        tokenizer.  *"custom" will use --tokenizer to select
                        the preregistered tokenizer. (default: auto)
  --trust-remote-code   Trust remote code from huggingface (default: False)
  --use-beam-search
  -h, --help            show this help message and exit

custom dataset options:
  --custom-output-len CUSTOM_OUTPUT_LEN
                        Number of output tokens per request, used only for
                        custom dataset. (default: 256)
  --custom-skip-chat-template
                        Skip applying chat template to prompt, used only for
                        custom dataset. (default: False)

sonnet dataset options:
  --sonnet-input-len SONNET_INPUT_LEN
                        Number of input tokens per request, used only for
                        sonnet dataset. (default: 550)
  --sonnet-output-len SONNET_OUTPUT_LEN
                        Number of output tokens per request, used only for
                        sonnet dataset. (default: 150)
  --sonnet-prefix-len SONNET_PREFIX_LEN
                        Number of prefix tokens per request, used only for
                        sonnet dataset. (default: 200)

sharegpt dataset options:
  --sharegpt-output-len SHAREGPT_OUTPUT_LEN
                        Output length for each request. Overrides the output
                        length from the ShareGPT dataset. (default: None)

random dataset options:
  --random-input-len RANDOM_INPUT_LEN
                        Number of input tokens per request, used only for
                        random sampling. (default: 1024)
  --random-output-len RANDOM_OUTPUT_LEN
                        Number of output tokens per request, used only for
                        random sampling. (default: 128)
  --random-prefix-len RANDOM_PREFIX_LEN
                        Number of fixed prefix tokens before the random
                        context in a request. The total input length is the
                        sum of `random-prefix-len` and a random context length
                        sampled from [input_len * (1 - range_ratio), input_len
                        * (1 + range_ratio)]. (default: 0)
  --random-range-ratio RANDOM_RANGE_RATIO
                        Range ratio for sampling input/output length, used
                        only for random sampling. Must be in the range [0, 1)
                        to define a symmetric sampling range[length * (1 -
                        range_ratio), length * (1 + range_ratio)]. (default:
                        0.0)

hf dataset options:
  --hf-output-len HF_OUTPUT_LEN
                        Output length for each request. Overrides the output
                        lengths from the sampled HF dataset. (default: None)
  --hf-split HF_SPLIT   Split of the HF dataset. (default: None)
  --hf-subset HF_SUBSET
                        Subset of the HF dataset. (default: None)

sampling parameters:
  --min-p MIN_P         Min-p sampling parameter. Only has effect on openai-
                        compatible backends. (default: None)
  --temperature TEMPERATURE
                        Temperature sampling parameter. Only has effect on
                        openai-compatible backends. If not specified, default
                        to greedy decoding (i.e. temperature==0.0). (default:
                        None)
  --top-k TOP_K         Top-k sampling parameter. Only has effect on openai-
                        compatible backends. (default: None)
  --top-p TOP_P         Top-p sampling parameter. Only has effect on openai-
                        compatible backends. (default: None)
